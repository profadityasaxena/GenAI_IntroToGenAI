{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2095bb4",
   "metadata": {},
   "source": [
    "# NLP Vectorization Pipeline\n",
    "This Jupyter notebook demonstrates four fundamental techniques for converting text into numerical features suitable for neural network models:\n",
    "1. **One‑Hot Encoding**\n",
    "2. **Bag‑of‑Words (BoW)**\n",
    "3. **Word Embeddings** (`nn.Embedding`)\n",
    "4. **Embedding Bags** (`nn.EmbeddingBag`)\n",
    "\n",
    "Each technique is applied to the same miniature corpus so you can observe and compare their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d184de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ['I like cats', 'I hate dogs', \"I'm impartial to hippos\"]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Imports and Sample Corpus\n",
    "# -----------------------------\n",
    "from collections import Counter  # For counting word frequencies if needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reproducibility\n",
    "# Set random seed for reproducibility (affects torch random ops, e.g., embedding initialization)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Sample corpus (three documents)\n",
    "corpus = [\n",
    "    \"I like cats\",\n",
    "    \"I hate dogs\",\n",
    "    \"I'm impartial to hippos\"\n",
    "]\n",
    "\n",
    "print(\"Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised Documents: [['i', 'like', 'cats'], ['i', 'hate', 'dogs'], ['im', 'impartial', 'to', 'hippos']]\n",
      "Vocabulary: {'cats': 0, 'dogs': 1, 'hate': 2, 'hippos': 3, 'i': 4, 'im': 5, 'impartial': 6, 'like': 7, 'to': 8}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# 2. Tokenisation and Vocabulary Build\n",
    "# -------------------------------------\n",
    "def tokenize(text):\n",
    "    # The replace method is used to replace all occurrences of a specified substring with another substring.\n",
    "    # Syntax: str.replace(old, new)\n",
    "    # In this case, it removes apostrophes by replacing \"'\" with \"\" (empty string).\n",
    "    return text.lower().replace(\"'\", \"\").split() \n",
    "\n",
    "# Tokenise corpus\n",
    "tokenised_docs = [tokenize(doc) for doc in corpus]\n",
    "print(\"Tokenised Documents:\", tokenised_docs)\n",
    "\n",
    "# Build vocabulary (word -> index), starting at 0\n",
    "all_tokens = [tok for doc in tokenised_docs for tok in doc]\n",
    "vocab = {tok: idx for idx, tok in enumerate(sorted(set(all_tokens)))}\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b786d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One‑Hot vectors for doc0:\n",
      " [[0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# 3. One‑Hot Encoding (per word)\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def one_hot(word):\n",
    "    vec = np.zeros(vocab_size, dtype=int)  # Create a zero vector of vocab size for one-hot encoding\n",
    "    vec[vocab[word]] = 1  # Set the index corresponding to the word in the vocab to 1 (one-hot encoding)\n",
    "    return vec\n",
    "\n",
    "# Example: one‑hot vectors for first document\n",
    "doc0_vectors = [one_hot(tok) for tok in tokenised_docs[0]]\n",
    "print(\"One‑Hot vectors for doc0:\\n\", np.array(doc0_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c2581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag‑of‑Words representations:\n",
      "Doc 0: [1 0 0 0 1 0 0 1 0]\n",
      "Doc 1: [0 1 1 0 1 0 0 0 0]\n",
      "Doc 2: [0 0 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4. Bag‑of‑Words (sum of one‑hot vectors per document)\n",
    "# ------------------------------------------------------\n",
    "def bow_vector(doc_tokens):\n",
    "    # For a given list of tokens in a document, compute the Bag-of-Words vector:\n",
    "    # 1. For each token, generate its one-hot vector using the one_hot() function.\n",
    "    # 2. Stack all one-hot vectors for the document and sum them along axis=0.\n",
    "    #    This results in a single vector of vocab_size length, where each position\n",
    "    #    contains the count of the corresponding word in the document.\n",
    "    vec = np.sum([one_hot(tok) for tok in doc_tokens], axis=0) \n",
    "    return vec\n",
    "\n",
    "bow_representations = [bow_vector(doc) for doc in tokenised_docs]  # Compute BoW vector for each document in the corpus\n",
    "print(\"Bag‑of‑Words representations:\")\n",
    "for i, vec in enumerate(bow_representations):\n",
    "    print(f\"Doc {i}: {vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8db6197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for first document (shape = [tokens, embedding_dim]):\n",
      " tensor([[ 0.5230,  0.9717, -0.2779, -0.6116, -0.5572, -0.9683,  0.8713, -0.0956],\n",
      "        [ 0.8854,  0.1824,  0.7864, -0.0579,  0.5667, -0.7098, -0.4875,  0.0501],\n",
      "        [ 0.5635,  1.8582,  1.0441, -0.8638,  0.8351, -0.3157,  0.2691,  0.0854]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# 5. Word Embeddings using nn.Embedding\n",
    "# --------------------------------------------\n",
    "embedding_dim = 8  # small dimension for demonstration\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Convert token lists to index tensors\n",
    "index_docs = [torch.tensor([vocab[tok] for tok in doc], dtype=torch.long) for doc in tokenised_docs]\n",
    "\n",
    "# Retrieve embeddings for first document\n",
    "embeddings_doc0 = embedding_layer(index_docs[0])\n",
    "print(\"Embeddings for first document (shape = [tokens, embedding_dim]):\\n\", embeddings_doc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ecfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat indices: tensor([4, 7, 0, 4, 2, 1, 5, 6, 8, 3])\n",
      "Offsets: tensor([0, 3, 6])\n",
      "EmbeddingBag outputs (shape = [docs, embedding_dim]):\n",
      " tensor([[-0.0471, -0.3956,  0.1453,  0.0946,  0.4031, -0.5658, -0.3008, -0.2628],\n",
      "        [ 0.0516,  0.3793,  0.8084, -0.1968,  0.6478, -0.1254, -0.4170,  0.2644],\n",
      "        [-0.8807,  0.4806,  0.3092,  0.2491,  0.0087,  0.1930,  1.0166,  0.9075]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 6. Embedding Bag (average of embeddings per doc)\n",
    "# --------------------------------------------------\n",
    "embedding_bag = nn.EmbeddingBag(num_embeddings=vocab_size,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                mode='mean')\n",
    "\n",
    "# Flatten all indices into one 1‑D tensor\n",
    "flat_indices = torch.cat(index_docs)\n",
    "# Offsets: starting indices of each document in flat_indices\n",
    "offsets = torch.tensor([0] + [len(d) for d in index_docs[:-1]]).cumsum(dim=0)\n",
    "\n",
    "print(\"Flat indices:\", flat_indices)\n",
    "print(\"Offsets:\", offsets)\n",
    "\n",
    "# Compute embedding bag representations\n",
    "bag_outputs = embedding_bag(flat_indices, offsets)\n",
    "print(\"EmbeddingBag outputs (shape = [docs, embedding_dim]):\\n\", bag_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4a7c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook illustrated how the same corpus is represented under four vectorisation schemes. You can now extend this notebook by training a classifier on top of any of these representations or experimenting with different embedding dimensions and pooling modes.\n",
    "\n",
    "---\n",
    "**Next Steps (Suggested Exercises)**\n",
    "1. Add a simple fully‑connected classifier on top of the Bag‑of‑Words vectors.\n",
    "2. Train the embedding + linear layers end‑to‑end on a larger labelled dataset.\n",
    "3. Compare model performance and runtime across the four representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
