{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2095bb4",
   "metadata": {},
   "source": [
    "# NLP Vectorization Pipeline\n",
    "This Jupyter notebook demonstrates four fundamental techniques for converting text into numerical features suitable for neural network models:\n",
    "1. **One‑Hot Encoding**\n",
    "2. **Bag‑of‑Words (BoW)**\n",
    "3. **Word Embeddings** (`nn.Embedding`)\n",
    "4. **Embedding Bags** (`nn.EmbeddingBag`)\n",
    "\n",
    "Each technique is applied to the same miniature corpus so you can observe and compare their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d184de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Imports and Sample Corpus\n",
    "# -----------------------------\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Sample corpus (three documents)\n",
    "corpus = [\n",
    "    \"I like cats\",\n",
    "    \"I hate dogs\",\n",
    "    \"I'm impartial to hippos\"\n",
    "]\n",
    "\n",
    "print(\"Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# 2. Tokenisation and Vocabulary Build\n",
    "# -------------------------------------\n",
    "def tokenize(text):\n",
    "    return text.lower().replace(\"'\", \"\").split()\n",
    "\n",
    "# Tokenise corpus\n",
    "tokenised_docs = [tokenize(doc) for doc in corpus]\n",
    "print(\"Tokenised Documents:\", tokenised_docs)\n",
    "\n",
    "# Build vocabulary (word -> index), starting at 0\n",
    "all_tokens = [tok for doc in tokenised_docs for tok in doc]\n",
    "vocab = {tok: idx for idx, tok in enumerate(sorted(set(all_tokens)))}\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 3. One‑Hot Encoding (per word)\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def one_hot(word):\n",
    "    vec = np.zeros(vocab_size, dtype=int)\n",
    "    vec[vocab[word]] = 1\n",
    "    return vec\n",
    "\n",
    "# Example: one‑hot vectors for first document\n",
    "doc0_vectors = [one_hot(tok) for tok in tokenised_docs[0]]\n",
    "print(\"One‑Hot vectors for doc0:\\n\", np.array(doc0_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4. Bag‑of‑Words (sum of one‑hot vectors per document)\n",
    "# ------------------------------------------------------\n",
    "def bow_vector(doc_tokens):\n",
    "    vec = np.sum([one_hot(tok) for tok in doc_tokens], axis=0)\n",
    "    return vec\n",
    "\n",
    "bow_representations = [bow_vector(doc) for doc in tokenised_docs]\n",
    "print(\"Bag‑of‑Words representations:\")\n",
    "for i, vec in enumerate(bow_representations):\n",
    "    print(f\"Doc {i}: {vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# 5. Word Embeddings using nn.Embedding\n",
    "# --------------------------------------------\n",
    "embedding_dim = 8  # small dimension for demonstration\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Convert token lists to index tensors\n",
    "index_docs = [torch.tensor([vocab[tok] for tok in doc], dtype=torch.long) for doc in tokenised_docs]\n",
    "\n",
    "# Retrieve embeddings for first document\n",
    "embeddings_doc0 = embedding_layer(index_docs[0])\n",
    "print(\"Embeddings for first document (shape = [tokens, embedding_dim]):\\n\", embeddings_doc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 6. Embedding Bag (average of embeddings per doc)\n",
    "# --------------------------------------------------\n",
    "embedding_bag = nn.EmbeddingBag(num_embeddings=vocab_size,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                mode='mean')\n",
    "\n",
    "# Flatten all indices into one 1‑D tensor\n",
    "flat_indices = torch.cat(index_docs)\n",
    "# Offsets: starting indices of each document in flat_indices\n",
    "offsets = torch.tensor([0] + [len(d) for d in index_docs[:-1]]).cumsum(dim=0)\n",
    "\n",
    "print(\"Flat indices:\", flat_indices)\n",
    "print(\"Offsets:\", offsets)\n",
    "\n",
    "# Compute embedding bag representations\n",
    "bag_outputs = embedding_bag(flat_indices, offsets)\n",
    "print(\"EmbeddingBag outputs (shape = [docs, embedding_dim]):\\n\", bag_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4a7c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook illustrated how the same corpus is represented under four vectorisation schemes. You can now extend this notebook by training a classifier on top of any of these representations or experimenting with different embedding dimensions and pooling modes.\n",
    "\n",
    "---\n",
    "**Next Steps (Suggested Exercises)**\n",
    "1. Add a simple fully‑connected classifier on top of the Bag‑of‑Words vectors.\n",
    "2. Train the embedding + linear layers end‑to‑end on a larger labelled dataset.\n",
    "3. Compare model performance and runtime across the four representations."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
