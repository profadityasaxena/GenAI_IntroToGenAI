{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b80a6d1",
   "metadata": {},
   "source": [
    "# Document Classification using Neural Networks and TorchText\n",
    "This notebook demonstrates how to implement a simple document classification model using PyTorch and TorchText. We will work with the AG News dataset and build a pipeline for:\n",
    "1. Text preprocessing and tokenization\n",
    "2. Embedding text using `nn.EmbeddingBag`\n",
    "3. Building a feedforward neural network\n",
    "4. Making predictions using `argmax` over logits\n",
    "5. Understanding logits, classes, and hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1823f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x221f0d3ffb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Imports and Setup\n",
    "import torch  # Import PyTorch core library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torchtext  # Import torchtext for NLP datasets and tools\n",
    "from torchtext.datasets import AG_NEWS  # Import AG_NEWS dataset\n",
    "from torchtext.data.utils import get_tokenizer  # Import tokenizer utility\n",
    "from torchtext.vocab import build_vocab_from_iterator  # Import vocab builder\n",
    "from torch.utils.data import DataLoader  # Import DataLoader for batching\n",
    "\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import time  # Import time module for timing\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)  # Set random seed for PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db515ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 95811\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load AG_NEWS Dataset and Tokenize\n",
    "train_iter = AG_NEWS(split='train')  # Load the AG_NEWS training dataset\n",
    "tokenizer = get_tokenizer('basic_english')  # Create a basic English tokenizer\n",
    "\n",
    "def yield_tokens(data_iter):  # Define a generator to yield tokens from dataset\n",
    "    for _, text in data_iter:  # Iterate over each (label, text) pair\n",
    "        yield tokenizer(text)  # Yield tokenized text\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])  # Build vocabulary from tokens, add <unk> special token\n",
    "# The <unk> (unknown) token is needed to handle words that are not present in the vocabulary.\n",
    "# When processing real-world text, it's common to encounter words that were not seen during vocabulary building.\n",
    "# If the model encounters such out-of-vocabulary (OOV) words, it needs a way to represent them.\n",
    "# By assigning a default index to \"<unk>\", any unknown word will be mapped to this token's embedding.\n",
    "# This prevents errors during inference and ensures the model can process any input text, even with unseen words.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown tokens\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))  # Print the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835f6782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices: [202, 16, 9, 9254, 179, 20726, 23, 4216, 7, 1096, 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pipeline to Encode Text as Tensor\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))  # Convert text to list of token indices using tokenizer and vocab\n",
    "label_pipeline = lambda x: int(x) - 1  # Convert label string to integer index starting from 0\n",
    "\n",
    "example_text = \"Google's quantum computer achieves new milestone in speed.\"  # Example text for preview\n",
    "print(\"Token indices:\", text_pipeline(example_text))  # Print token indices for the example text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5ada1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Batch Function with Offsets (for EmbeddingBag)\n",
    "def collate_batch(batch):  # Define function to collate a batch for DataLoader\n",
    "    label_list, text_list, offsets = [], [], [0]  # Initialize lists for labels, texts, and offsets\n",
    "    for (_label, _text) in batch:  # Iterate over each sample in the batch\n",
    "        label_list.append(label_pipeline(_label))  # Process and append label\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # Tokenize and convert text to tensor\n",
    "        text_list.append(processed_text)  # Append processed text tensor\n",
    "        offsets.append(processed_text.size(0))  # Append length of processed text to offsets\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)  # Convert label list to tensor\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # Compute cumulative sum of offsets (excluding last)\n",
    "    text_tensor = torch.cat(text_list)  # Concatenate all text tensors into one tensor\n",
    "    return label_tensor, text_tensor, offsets  # Return label tensor, text tensor, and offsets\n",
    "\n",
    "# Create DataLoader\n",
    "train_iter = AG_NEWS(split='train')  # Reload AG_NEWS training dataset\n",
    "dataloader = DataLoader(list(train_iter)[:1000], batch_size=8, shuffle=True, collate_fn=collate_batch)  # Create DataLoader with custom collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d96ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(95811, 64, mode='mean')\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define Model Architecture (EmbeddingBag + Linear Layer)\n",
    "class TextClassificationModel(nn.Module):  # Define the model class inheriting from nn.Module\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):  # Initialize model with vocab size, embedding dim, and number of classes\n",
    "        super(TextClassificationModel, self).__init__()  # Call parent constructor\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)  # EmbeddingBag layer for efficient embedding lookup\n",
    "        self.fc = nn.Linear(embed_dim, num_class)  # Linear layer for classification\n",
    "        self.init_weights()  # Initialize weights\n",
    "\n",
    "    def init_weights(self):  # Method to initialize weights\n",
    "        initrange = 0.5  # Set initialization range\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)  # Uniform init for embedding weights\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)  # Uniform init for linear weights\n",
    "        self.fc.bias.data.zero_()  # Zero init for linear bias\n",
    "\n",
    "    def forward(self, text, offsets):  # Define forward pass\n",
    "        embedded = self.embedding(text, offsets)  # Get embeddings for input text\n",
    "        return self.fc(embedded)  # Pass embeddings through linear layer\n",
    "\n",
    "num_classes = 4  # Number of output classes\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "embed_dim = 64  # Dimension of embeddings\n",
    "\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes)  # Instantiate the model\n",
    "print(model)  # Print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[ 0.1303,  0.1744,  0.0400, -0.0927],\n",
      "        [ 0.1418,  0.0776, -0.0377, -0.1060],\n",
      "        [ 0.0352,  0.3715, -0.0508, -0.1522],\n",
      "        [-0.0834, -0.0282, -0.1419,  0.0064],\n",
      "        [ 0.2335,  0.1354,  0.0988,  0.0340],\n",
      "        [-0.0714,  0.0370,  0.0396, -0.1004],\n",
      "        [ 0.0967, -0.1384, -0.0711,  0.0736],\n",
      "        [ 0.1472,  0.0940, -0.2131, -0.0617]], grad_fn=<AddmmBackward0>)\n",
      "Predicted classes: tensor([1, 0, 1, 3, 0, 2, 0, 0])\n",
      "True labels: tensor([0, 3, 0, 2, 3, 2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Make Predictions with Argmax\n",
    "for labels, text, offsets in dataloader:  # Iterate over batches from the dataloader\n",
    "    outputs = model(text, offsets)  # Get model outputs (logits) for the batch\n",
    "    predictions = torch.argmax(outputs, dim=1)  # Get predicted class indices using argmax\n",
    "    print(\"Logits:\\n\", outputs)  # Print the raw logits\n",
    "    print(\"Predicted classes:\", predictions)  # Print the predicted class indices\n",
    "    print(\"True labels:\", labels)  # Print the true labels for comparison\n",
    "    break  # Process only the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284da632",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you:\n",
    "- Loaded the AG_NEWS dataset using TorchText\n",
    "- Built a vocabulary and tokenized the data\n",
    "- Used `nn.EmbeddingBag` to aggregate word embeddings efficiently\n",
    "- Built a simple classifier with a linear output layer\n",
    "- Used the `argmax` function to predict classes from logits\n",
    "\n",
    "**Next steps**: Train the model using a loss function like `CrossEntropyLoss` and an optimizer like `SGD` or `Adam`, and evaluate it on a test set.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
