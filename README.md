# Introduction to Large Language Models (LLMs) and Generative AI

## Course Overview

This course introduces the foundational concepts, architectures, and applications of **Large Language Models (LLMs)** and **Generative AI**. It is designed to bridge the gap between theoretical understanding and practical implementation, enabling students to engage with modern generative systems including GPT, BERT, T5, diffusion models, and other foundation models.

Students will explore the mathematical, algorithmic, and engineering principles behind language models and their capacity for generation, reasoning, summarization, translation, and multimodal learning.

## Learning Objectives

By the end of this course, students will be able to:

- Understand the architecture and working principles of transformers and attention mechanisms.
- Describe the evolution of LLMs from word embeddings to autoregressive models.
- Implement and fine-tune pretrained models using popular frameworks (e.g., HuggingFace Transformers, OpenAI API).
- Analyze ethical, legal, and social implications of generative AI in real-world deployments.
- Develop basic applications using LLMs such as chatbots, summarizers, and creative content generators.

## Course Modules

1. **Introduction to Generative AI**
   - What is Generative AI?
   - Historical context and key breakthroughs

2. **From Language Models to Large Language Models**
   - N-gram models, RNNs, LSTMs
   - Word2Vec, GloVe, ELMo

3. **Transformers and Self-Attention**
   - Vaswani et al. (2017) Transformer architecture
   - Positional encoding, multi-head attention

4. **Modern LLMs and Pretraining Objectives**
   - BERT (masked language modeling)
   - GPT series (causal language modeling)
   - T5 and sequence-to-sequence architectures

5. **Training and Fine-Tuning LLMs**
   - Tokenization, embeddings, model scaling
   - Transfer learning and parameter-efficient fine-tuning (LoRA, PEFT)

6. **Generative Applications and Prompt Engineering**
   - Text generation, summarization, translation
   - Zero-shot, few-shot, and chain-of-thought prompting

7. **Multimodal Generative AI**
   - Vision-language models (e.g., CLIP, DALL·E)
   - Text-to-image and video generation

8. **Risks, Bias, and Responsible AI**
   - Hallucination, toxicity, privacy concerns
   - AI safety and alignment research

## Prerequisites

- Basic understanding of Python programming
- Familiarity with linear algebra, probability, and machine learning fundamentals
- Prior experience with deep learning frameworks (TensorFlow or PyTorch) is helpful but not mandatory

## Technologies and Tools

- Python 3.10+
- PyTorch / TensorFlow
- HuggingFace Transformers
- OpenAI API / Anthropic / Cohere (optional)
- Jupyter Notebooks / Google Colab

## Assessment and Evaluation

- **Weekly Labs** – Practical tasks using real LLM APIs and models
- **Midterm Project** – Mini research or application project (e.g., chatbot, summarizer)
- **Final Project** – Group-based or individual project on LLM application, fine-tuning, or ethical analysis
- **Participation** – Discussion of readings and current trends in generative AI

## References

- Vaswani et al., "Attention is All You Need", *NeurIPS*, 2017. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  
- Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", *NAACL*, 2019. [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)  
- Brown et al., "Language Models are Few-Shot Learners", *NeurIPS*, 2020. [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)  
- Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", *JMLR*, 2020. [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)

## License

This course material is licensed under the **Creative Commons Attribution-NonCommercial 4.0 International License**.

---

For academic inquiries, contact:  
**Aditya Saxena**  
Toronto Metropolitan University  
Email: aditya.saxena@torontomu.ca
# GenAI_IntroToGenAI
