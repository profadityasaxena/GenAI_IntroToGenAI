{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc079f4",
   "metadata": {},
   "source": [
    "# MINI-PROJECT - Text Tokenizer\n",
    "\n",
    "In this mini project, we will build a text tokenizer with multiple approaches like:\n",
    "\n",
    "1. Word Based\n",
    "2. Character Based\n",
    "3. Sub-word Based \n",
    "- WordPiece - evaluates the benefits and drawbacks of splitting and merging two symbols\n",
    "- Unigram - Breaks text into smaller pieces / Narrows down a large list of possibilities based on frequency of appearance\n",
    "- SentencePiece - Segments text into manageable partsand assign unique IDs.\n",
    "4. Adding special tokens like <bos> and <eos> at the beginning of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824b5b0",
   "metadata": {},
   "source": [
    "### EXAMPLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce53cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'{sys.executable}' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade --force-reinstall torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9d9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iam_a\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cpu\n",
      "CUDA available: False\n",
      "Input IDs:\n",
      " tensor([[  101,  2312,  2653,  4275,  1006,  2222,  5244,  1007,  2024, 17903,\n",
      "          2129,  6681,  3305,  2653,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"Large Language Models (LLMs) are transforming how machines understand language.\"\n",
    "\n",
    "# Tokenize and return PyTorch tensors\n",
    "tokens = tokenizer(\n",
    "    text,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=32,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Output the tokenized tensors\n",
    "print(\"Input IDs:\\n\", tokens['input_ids'])\n",
    "print(\"Attention Mask:\\n\", tokens['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b8d86",
   "metadata": {},
   "source": [
    "### 1. Word-Based Tokenizer\n",
    "Motivation\n",
    "The simplest form of tokenization—splitting by whitespace—provides an intuitive way to represent language. It’s historically been the foundation for bag-of-words models.\n",
    "\n",
    "Significance\n",
    "Efficient for small datasets and classical ML tasks (e.g., sentiment analysis, topic modeling). However, it fails on out-of-vocabulary (OOV) words and does not handle morphological variants.\n",
    "\n",
    "Practical Usage\n",
    "Used in early NLP pipelines (e.g., TF-IDF vectorizers, early RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170c9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'crucial', 'in', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenization is crucial in NLP.\"\n",
    "tokens = text.split()\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6e19b",
   "metadata": {},
   "source": [
    "### 2. Character-Based Tokenizer\n",
    "Motivation\n",
    "By decomposing into characters, the model can handle any string, including rare or novel words.\n",
    "\n",
    "Significance\n",
    "This approach avoids OOV issues and captures morphological patterns but at the cost of longer sequences.\n",
    "\n",
    "Practical Usage\n",
    "Used in text generation tasks and when modeling fine-grained linguistic structures (e.g., OCR, speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizer\"\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb881bb",
   "metadata": {},
   "source": [
    "### 3. Subword-Based Tokenizer\n",
    "Subword methods provide a balance between the flexibility of character tokenization and the compactness of word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9234ca",
   "metadata": {},
   "source": [
    "#### 3.1 WordPiece\n",
    "Motivation\n",
    "Originally developed for BERT, WordPiece merges frequent symbol pairs to build a vocabulary.\n",
    "\n",
    "Significance\n",
    "Efficient handling of rare and compound words (e.g., unaffordable → un ##afford ##able).\n",
    "\n",
    "Practical Usage\n",
    "Used in BERT and other Transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49482438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['una', '##ff', '##ord', '##able', 'housing']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.tokenize(\"unaffordable housing\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6937d79",
   "metadata": {},
   "source": [
    "#### 3.2 Unigram\n",
    "Motivation\n",
    "Instead of merging, it selects subwords from a fixed vocabulary to maximize likelihood.\n",
    "\n",
    "Significance\n",
    "Provides probabilistic coverage and optimal subword selection.\n",
    "\n",
    "Practical Usage\n",
    "Used in Google's T5 and XLNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0403da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['<bos>', 'un', 'a', 'f', 'for', 'd', 'a', 'b', 'l', 'e', 'h', 'o', 'u', 's', 'i', 'ng', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Step 1: Sample data\n",
    "corpus = [\n",
    "    \"unaffordable housing\",\n",
    "    \"natural language processing is fun\",\n",
    "    \"tokenization improves NLP performance\",\n",
    "]\n",
    "\n",
    "# Step 2: Initialize a Unigram model\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "# Step 3: Add normalizer and pre-tokenizer\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Step 4: Define trainer\n",
    "trainer = trainers.UnigramTrainer(vocab_size=100, show_progress=True)\n",
    "\n",
    "# Step 5: Train tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# (Optional) Add special tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <sep> $B:1 <eos>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", 1),\n",
    "        (\"<eos>\", 2),\n",
    "        (\"<sep>\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Step 6: Save the tokenizer to file\n",
    "tokenizer.save(\"unigram_tokenizer.json\")\n",
    "\n",
    "# Step 7: Reload and test\n",
    "tokenizer = Tokenizer.from_file(\"unigram_tokenizer.json\")\n",
    "output = tokenizer.encode(\"unaffordable housing\")\n",
    "print(\"Tokens:\", output.tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8d9b1",
   "metadata": {},
   "source": [
    "This code demonstrates how to train a Unigram-based text tokenizer using the tokenizers library. It begins by importing essential modules for tokenization, normalization, pre-tokenization, and processing. A small sample corpus of text sentences is provided to serve as training data. A Unigram model is then initialized, which is designed to break words into meaningful subword units based on frequency and likelihood. The text is normalized using the NFKC standard to handle variations in characters and is split into words using whitespace pre-tokenization. A trainer is defined with a vocabulary size of 100, guiding how many subword units the tokenizer should learn. The tokenizer is then trained on the provided corpus using this trainer. To support downstream language models, special tokens such as <bos> (beginning of sentence), <eos> (end of sentence), and <sep> (separator) are added using a template post-processor. The trained tokenizer is saved to a JSON file for reuse, and finally, it is reloaded and used to encode a new input sentence, printing the resulting subword tokens as output. This workflow encapsulates a complete tokenizer training pipeline for subword-based NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be67a4c",
   "metadata": {},
   "source": [
    "### 3.3 SentencePiece\n",
    "Motivation\n",
    "Builds subword units from raw text without requiring pre-tokenization. It treats whitespace as a normal character.\n",
    "\n",
    "Significance\n",
    "Language-agnostic and used in multilingual settings.\n",
    "\n",
    "Practical Usage\n",
    "Used in models like ALBERT, mBART, and T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd3f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁', 'u', 'n', 'a', 'f', 'f', 'o', 'r', 'd', 'a', 'b', 'l', 'e', '▁', 'h', 'o', 'u', 's', 'ing']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Step 1: Write your training corpus to a file\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    corpus = [\n",
    "        \"unaffordable housing\",\n",
    "        \"natural language processing is fun\",\n",
    "        \"tokenization improves NLP performance\",\n",
    "        \"deep learning models require a lot of data\",\n",
    "        \"generative models can produce realistic text\",\n",
    "        \"neural networks learn from examples\",\n",
    "        \"transformers use attention mechanisms\",\n",
    "        \"language models are pre-trained on massive datasets\",\n",
    "        \"BERT and GPT are popular NLP architectures\",\n",
    "        \"machine translation is a classic NLP task\",\n",
    "        \"text summarization condenses information\",\n",
    "        \"question answering systems understand queries\",\n",
    "        \"word embeddings capture semantic meaning\",\n",
    "        \"subword tokenization handles rare words\",\n",
    "        \"sentencepiece segments text effectively\",\n",
    "        \"training tokenizers requires representative data\",\n",
    "        \"AI systems benefit from clean tokenized input\"\n",
    "    ]\n",
    "    f.write(\"\\n\".join(corpus))\n",
    "\n",
    "# Step 2: Train the SentencePiece model (Unigram with vocab size 30)\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix='spm',\n",
    "    vocab_size=50,\n",
    "    model_type='unigram',\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    pad_id=0,\n",
    "    unk_id=3\n",
    ")\n",
    "\n",
    "# Step 3: Load the trained model and tokenize text\n",
    "sp = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "tokens = sp.encode(\"unaffordable housing\", out_type=str)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
