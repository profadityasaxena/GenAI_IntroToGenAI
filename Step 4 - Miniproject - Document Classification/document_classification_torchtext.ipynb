{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b80a6d1",
   "metadata": {},
   "source": [
    "# Document Classification using Neural Networks and TorchText\n",
    "This notebook demonstrates how to implement a simple document classification model using PyTorch and TorchText. We will work with the AG News dataset and build a pipeline for:\n",
    "1. Text preprocessing and tokenization\n",
    "2. Embedding text using `nn.EmbeddingBag`\n",
    "3. Building a feedforward neural network\n",
    "4. Making predictions using `argmax` over logits\n",
    "5. Understanding logits, classes, and hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1823f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db515ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load AG_NEWS Dataset and Tokenize\n",
    "train_iter = AG_NEWS(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Pipeline to Encode Text as Tensor\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # Adjust labels to start from 0\n",
    "\n",
    "# Preview a sample\n",
    "example_text = \"Google's quantum computer achieves new milestone in speed.\"\n",
    "print(\"Token indices:\", text_pipeline(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ada1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Batch Function with Offsets (for EmbeddingBag)\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_tensor = torch.cat(text_list)\n",
    "    return label_tensor, text_tensor, offsets\n",
    "\n",
    "# Create DataLoader\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(list(train_iter)[:1000], batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d96ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Model Architecture (EmbeddingBag + Linear Layer)\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "num_classes = 4\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make Predictions with Argmax\n",
    "# Fetch a single batch\n",
    "for labels, text, offsets in dataloader:\n",
    "    outputs = model(text, offsets)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    print(\"Logits:\\n\", outputs)\n",
    "    print(\"Predicted classes:\", predictions)\n",
    "    print(\"True labels:\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284da632",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you:\n",
    "- Loaded the AG_NEWS dataset using TorchText\n",
    "- Built a vocabulary and tokenized the data\n",
    "- Used `nn.EmbeddingBag` to aggregate word embeddings efficiently\n",
    "- Built a simple classifier with a linear output layer\n",
    "- Used the `argmax` function to predict classes from logits\n",
    "\n",
    "**Next steps**: Train the model using a loss function like `CrossEntropyLoss` and an optimizer like `SGD` or `Adam`, and evaluate it on a test set.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
